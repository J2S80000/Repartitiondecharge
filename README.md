# MongoDB Sharded Cluster - Architecture ComplÃ¨te

## ğŸ“Š Vue d'ensemble de l'architecture

Ce projet implÃ©mente un **cluster MongoDB shardÃ©** complet avec:
- **3 shards** (replica sets A, B, C) pour la rÃ©partition des donnÃ©es
- **1 serveur de configuration** (historique) pour les mÃ©tadonnÃ©es du cluster
- **2 routeurs mongos** pour diriger les requÃªtes vers les shards appropriÃ©s
- **1 interface AdminMongo** pour la gestion graphique

---

## ğŸ—ï¸ Architecture dÃ©taillÃ©e

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  Routeur 1  â”‚     â”‚  Routeur 2  â”‚
                    â”‚  (mongos)   â”‚     â”‚  (mongos)   â”‚
                    â”‚  :27040     â”‚     â”‚  :27041     â”‚
                    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                           â”‚                   â”‚
                           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                     â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚                                 â”‚
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚   Config Server   â”‚          â”‚   AdminMongo :1234   â”‚
          â”‚    (historique)   â”‚          â”‚   Interface Web      â”‚
          â”‚  3 nÅ“uds :27030-32â”‚          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚           â”‚           â”‚
   â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â” â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â” â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”
   â”‚ Shard A â”‚ â”‚Shard B â”‚ â”‚Shard C â”‚
   â”‚         â”‚ â”‚        â”‚ â”‚        â”‚
   â”‚principalâ”‚ â”‚principalâ”‚ â”‚principalâ”‚
   â”‚ :27017  â”‚ â”‚ :27021 â”‚ â”‚ :27025 â”‚
   â”‚         â”‚ â”‚        â”‚ â”‚        â”‚
   â”‚3 second.â”‚ â”‚3 secondâ”‚ â”‚3 secondâ”‚
   â”‚27018-20 â”‚ â”‚27022-24â”‚ â”‚27026-28â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“¦ Composants du cluster

### ğŸ”· Shard A (replSet_a)
- **principal_a** â†’ Port 27017 (PRIMARY)
- **secondaire_a_1** â†’ Port 27018 (SECONDARY)
- **secondaire_a_2** â†’ Port 27019 (SECONDARY)
- **secondaire_a_3** â†’ Port 27020 (SECONDARY)

### ğŸ”· Shard B (replSet_b)
- **principal_b** â†’ Port 27021 (PRIMARY)
- **secondaire_b_1** â†’ Port 27022 (SECONDARY)
- **secondaire_b_2** â†’ Port 27023 (SECONDARY)
- **secondaire_b_3** â†’ Port 27024 (SECONDARY)

### ğŸ”· Shard C (replSet_c)
- **principal_c** â†’ Port 27025 (PRIMARY)
- **secondaire_c_1** â†’ Port 27026 (SECONDARY)
- **secondaire_c_2** â†’ Port 27027 (SECONDARY)
- **secondaire_c_3** â†’ Port 27028 (SECONDARY)

### ğŸ“š Config Server (configReplSet)
- **historique_1** â†’ Port 27030 (Config Server)
- **historique_2** â†’ Port 27031 (Config Server)
- **historique_3** â†’ Port 27032 (Config Server)

### ğŸ”€ Routeurs mongos
- **routeur_1** â†’ Port 27040 (Query Router)
- **routeur_2** â†’ Port 27041 (Query Router)

### ğŸŒ Interface Web
- **adminmongo** â†’ Port 1234 (http://localhost:1234)

---

## ğŸš€ DÃ©marrage du cluster

### 1ï¸âƒ£ DÃ©marrer tous les services
```powershell
docker-compose up -d
```

### 2ï¸âƒ£ VÃ©rifier l'Ã©tat du cluster
```powershell
# VÃ©rifier que tous les containers sont actifs
docker-compose ps

# Voir les logs d'initialisation
docker-compose logs setup-sharding
```

### 3ï¸âƒ£ VÃ©rifier le sharding
```powershell
# Se connecter au routeur
docker exec -it routeur_1 mongosh

# Dans le shell MongoDB :
sh.status()         # Ã‰tat du cluster shardÃ©
sh.getShards()      # Liste des shards
```

---

## ğŸ“ Commandes utiles

### Se connecter aux diffÃ©rents composants

```powershell
# Routeur 1 (point d'entrÃ©e principal)
docker exec -it routeur_1 mongosh

# Routeur 2
docker exec -it routeur_2 mongosh

# Shard A (primary)
docker exec -it principal_a mongosh

# Shard B (primary)
docker exec -it principal_b mongosh

# Shard C (primary)
docker exec -it principal_c mongosh

# Config server
docker exec -it historique_1 mongosh --port 27019
```

### Tester le sharding

```powershell
# Se connecter au routeur
docker exec -it routeur_1 mongosh

# Dans le shell MongoDB :
use testdb

# Activer le sharding sur la base
sh.enableSharding("testdb")

# CrÃ©er une collection shardÃ©e avec clÃ© de sharding
sh.shardCollection("testdb.users", { _id: "hashed" })

# InsÃ©rer des donnÃ©es (elles seront automatiquement rÃ©parties)
for (let i = 0; i < 10000; i++) {
  db.users.insertOne({ 
    name: "User" + i, 
    age: Math.floor(Math.random() * 80) + 18,
    email: "user" + i + "@example.com"
  })
}

# VÃ©rifier la distribution
sh.status()
db.users.getShardDistribution()
```

### ArrÃªter le cluster

```powershell
# ArrÃªt propre
docker-compose down

# ArrÃªt et suppression des volumes (âš ï¸ perte de donnÃ©es)
docker-compose down -v
```

---

## ğŸ” Monitoring et diagnostic

### VÃ©rifier l'Ã©tat des replica sets

```powershell
# Replica Set A
docker exec principal_a mongosh --eval "rs.status()" --quiet

# Replica Set B
docker exec principal_b mongosh --eval "rs.status()" --quiet

# Replica Set C
docker exec principal_c mongosh --eval "rs.status()" --quiet

# Config Server
docker exec historique_1 mongosh --port 27019 --eval "rs.status()" --quiet
```

### Voir les logs en temps rÃ©el

```powershell
# Tous les services
docker-compose logs -f

# Un service spÃ©cifique
docker-compose logs -f routeur_1
docker-compose logs -f principal_a
```

---
---

## âœ… TESTS ET VALIDATION

Cette section documente les tests rÃ©alisÃ©s pour valider le fonctionnement complet du cluster shardÃ©.

### ğŸ“‹ Test 1 : Import de donnÃ©es rÃ©elles (431 livres)

**Objectif** : DÃ©montrer la distribution automatique des donnÃ©es sur les 3 shards.

#### Ã‰tape 1 : Activation du sharding sur la base
```powershell
docker exec routeur_1 mongosh --quiet --eval "sh.enableSharding('paris')"
```

#### Ã‰tape 2 : Sharding de la collection avec clÃ© hashed
```powershell
docker exec routeur_1 mongosh --quiet --eval "sh.shardCollection('paris.books', {_id: 'hashed'})"
```

#### Ã‰tape 3 : Import des donnÃ©es VIA LE ROUTEUR
```powershell
# Copier le fichier JSON vers le routeur
docker cp "C:\Users\jessy\Documents\books.json" routeur_1:/tmp/books.json

# Importer via mongoimport
docker exec routeur_1 mongoimport --db paris --collection books --file /tmp/books.json
```

**RÃ©sultat** :
```
2025-10-16T11:29:43.903+0000    connected to: mongodb://localhost/
2025-10-16T11:29:43.997+0000    431 document(s) imported successfully. 0 document(s) failed to import.
```

âš ï¸ **IMPORTANT** : Toujours importer via le **routeur** (mongos), jamais directement sur un shard !

#### Ã‰tape 4 : VÃ©rification de la distribution des donnÃ©es
```powershell
docker exec routeur_1 mongosh --quiet --eval "use paris" --eval "db.books.getShardDistribution()"
```

**RÃ©sultat obtenu** :
```
Shard replSet_c :
  data: '137KiB'
  docs: 130 (30.16%)
  chunks: 1

Shard replSet_a :
  data: '190KiB'
  docs: 150 (34.80%)
  chunks: 1

Shard replSet_b :
  data: '176KiB'
  docs: 151 (35.03%)
  chunks: 1

Totals :
  data: '505KiB'
  docs: 431
  chunks: 3
```

âœ… **Validation** : Les 431 documents sont **Ã©quitablement rÃ©partis** sur les 3 shards (~33% chacun).

---

### ğŸ—„ï¸ Test 2 : VÃ©rification des Config Servers (Historique)

**Objectif** : Confirmer que les mÃ©tadonnÃ©es du cluster sont bien enregistrÃ©es et rÃ©pliquÃ©es sur les 3 config servers.

#### VÃ©rification sur historique_1
```powershell
docker exec historique_1 mongosh --port 27019 --quiet --eval "use config" --eval "print('Bases shardees enregistrees:'); db.databases.find().forEach(d => print('  - ' + d._id + ' (primary: ' + d.primary + ')'))"
```

**RÃ©sultat** :
```
Bases shardees enregistrees:
  - paris (primary: replSet_b)
```

#### VÃ©rification sur historique_2
```powershell
docker exec historique_2 mongosh --port 27019 --quiet --eval "use config" --eval "print('Bases shardees enregistrees:'); db.databases.find().forEach(d => print('  - ' + d._id + ' (primary: ' + d.primary + ')'))"
```

**RÃ©sultat** :
```
Bases shardees enregistrees:
  - paris (primary: replSet_b)
```

#### VÃ©rification sur historique_3
```powershell
docker exec historique_3 mongosh --port 27019 --quiet --eval "use config" --eval "print('Bases shardees enregistrees:'); db.databases.find().forEach(d => print('  - ' + d._id + ' (primary: ' + d.primary + ')'))"
```

**RÃ©sultat** :
```
Bases shardees enregistrees:
  - paris (primary: replSet_b)
```

âœ… **Validation** : Les mÃ©tadonnÃ©es sont **identiques sur les 3 config servers**, prouvant la rÃ©plication fonctionnelle.

**Note** : `primary: replSet_b` signifie que replSet_b est le "primary shard" pour les collections **non shardÃ©es** de cette base. Cela n'affecte PAS la distribution des collections shardÃ©es comme `paris.books`.

---

### ğŸ”„ Test 3 : Failover automatique (Haute disponibilitÃ©)

**Objectif** : DÃ©montrer qu'en cas de panne du PRIMARY, un SECONDARY est automatiquement promu.

#### Ã‰tat initial : principal_a est PRIMARY
```powershell
docker exec -it principal_a mongosh
```

**RÃ©sultat** :
```
Connecting to: mongodb://127.0.0.1:27017/?directConnection=true
Using MongoDB: 8.0.13

replSet_a [direct: primary] test>
```

âœ… `principal_a` est bien PRIMARY du replica set `replSet_a`.

#### Simulation de panne
```powershell
# ArrÃªter le container principal_a
docker stop principal_a

# Attendre l'Ã©lection (~10-15 secondes)
Start-Sleep -Seconds 15
```

#### VÃ©rification : un SECONDARY devient PRIMARY
```powershell
docker exec -it secondaire_a_1 mongosh
```

**RÃ©sultat aprÃ¨s Ã©lection** :
```
Connecting to: mongodb://127.0.0.1:27017/?directConnection=true
Using MongoDB: 8.0.13

replSet_a [direct: primary] test>
```

âœ… **Validation** : `secondaire_a_1` a Ã©tÃ© **automatiquement promu PRIMARY** !

#### VÃ©rification du statut du replica set
```powershell
docker exec secondaire_a_1 mongosh --eval "rs.status()" --quiet | Select-String "stateStr"
```

**RÃ©sultat attendu** :
```
stateStr: 'DOWN'       â† principal_a (arrÃªtÃ©)
stateStr: 'PRIMARY'    â† secondaire_a_1 (nouveau PRIMARY)
stateStr: 'SECONDARY'  â† secondaire_a_2
stateStr: 'SECONDARY'  â† secondaire_a_3
```

#### Restauration
```powershell
# RedÃ©marrer principal_a
docker start principal_a

# Il redeviendra SECONDARY automatiquement
# (ou PRIMARY si vous le configurez avec priority plus Ã©levÃ©e)
```

âœ… **Conclusion** : Le cluster tolÃ¨re la **panne d'un nÅ“ud par replica set** sans interruption de service.

#### Test approfondi : RÃ©plication et restrictions d'Ã©criture

**Contexte** : `principal_a` est arrÃªtÃ©, `secondaire_a_1` est devenu PRIMARY.

##### 1. VÃ©rifier les donnÃ©es sur le nouveau PRIMARY
```powershell
docker exec -it secondaire_a_1 mongosh
```

```javascript
use paris
db.books.countDocuments()
// RÃ©sultat : 150
```

âœ… Les **150 livres** du shard A sont toujours accessibles !

##### 2. VÃ©rifier la rÃ©plication sur les SECONDARY
```powershell
docker exec -it secondaire_a_2 mongosh
```

```javascript
use paris
db.books.countDocuments()
// RÃ©sultat : 150
```

âœ… Les donnÃ©es sont **parfaitement rÃ©pliquÃ©es** sur tous les nÅ“uds.

##### 3. Afficher quelques livres
```javascript
db.books.find({}, { _id: 1, title: 1 }).limit(5)
```

**RÃ©sultat** :
```javascript
[
  { _id: 29, title: 'jQuery in Action' },
  { _id: 63, title: 'POJOs in Action' },
  { _id: 67, title: 'Wicket in Action' },
  { _id: 72, title: 'SCWCD Exam Study Kit Second Edition' },
  { _id: 132, title: 'Up to Speed with Swing, Second Edition' }
]
```

##### 4. Tentative de suppression sur un SECONDARY (âŒ Ã‰CHOUE)
```javascript
// Toujours connectÃ© sur secondaire_a_2 (SECONDARY)
db.books.deleteOne({ title: "Wicket in Action" })
```

**RÃ©sultat** :
```
MongoServerError[NotWritablePrimary]: not primary
```

âš ï¸ **RÃ¨gle importante** : Les **Ã©critures sont INTERDITES** sur les SECONDARY !
- Les SECONDARY sont en **lecture seule** (read-only)
- Seul le PRIMARY accepte les Ã©critures

##### 5. Suppression rÃ©ussie sur le PRIMARY
```powershell
# Se connecter au PRIMARY actuel (secondaire_a_1)
docker exec -it secondaire_a_1 mongosh
```

```javascript
use paris
db.books.deleteOne({ title: "Wicket in Action" })
```

**RÃ©sultat** :
```javascript
{ acknowledged: true, deletedCount: 1 }
```

âœ… Suppression rÃ©ussie sur le PRIMARY !

##### 6. VÃ©rification du comptage aprÃ¨s suppression
```javascript
db.books.countDocuments()
// RÃ©sultat : 149 (150 - 1)
```

##### 7. VÃ©rification de la rÃ©plication de la suppression
```powershell
docker exec -it secondaire_a_2 mongosh
```

```javascript
use paris
db.books.countDocuments()
// RÃ©sultat : 149
```

âœ… La **suppression est automatiquement rÃ©pliquÃ©e** sur tous les SECONDARY !

**LeÃ§ons apprises** :
- ğŸ”’ **Ã‰criture** : Uniquement sur PRIMARY
- ğŸ“– **Lecture** : PRIMARY + tous les SECONDARY (avec `rs.secondaryOk()` si besoin)
- ğŸ”„ **RÃ©plication** : Automatique et instantanÃ©e (quelques millisecondes)
- ğŸš€ **Failover** : Promotion automatique d'un SECONDARY en PRIMARY

---

### ğŸ“Š Test 4 : RequÃªtes via le routeur

**Objectif** : VÃ©rifier que les requÃªtes passent correctement par le routeur et interrogent les bons shards.

#### Compter tous les documents
```powershell
docker exec routeur_1 mongosh --quiet --eval "use paris" --eval "db.books.countDocuments()"
```

**RÃ©sultat** : `431` (le routeur interroge les 3 shards et additionne)

#### Rechercher un document spÃ©cifique
```powershell
docker exec routeur_1 mongosh --quiet --eval "use paris" --eval "db.books.findOne()"
```

**RÃ©sultat** : Retourne 1 document (le routeur va chercher sur UN seul shard grÃ¢ce au hash de `_id`)

#### VÃ©rifier la prÃ©sence des donnÃ©es sur chaque shard individuellement
```powershell
# Sur shard A
docker exec principal_a mongosh --quiet --eval "use paris" --eval "db.books.countDocuments()"
# RÃ©sultat : 150

# Sur shard B
docker exec principal_b mongosh --quiet --eval "use paris" --eval "db.books.countDocuments()"
# RÃ©sultat : 151

# Sur shard C
docker exec principal_c mongosh --quiet --eval "use paris" --eval "db.books.countDocuments()"
# RÃ©sultat : 130
```

âœ… **Total** : 150 + 151 + 130 = **431 documents** âœ“

---

### ğŸ§ª Script de test automatique

ExÃ©cutez le script PowerShell fourni pour tester automatiquement les config servers :

```powershell
.\test_historique.ps1
```

Ce script vÃ©rifie :
- âœ… Statut des 3 config servers (historique_1/2/3)
- âœ… Ã‰tat du replica set configReplSet (1 PRIMARY + 2 SECONDARY)
- âœ… PrÃ©sence des 3 shards dans les mÃ©tadonnÃ©es
- âœ… Collections du cluster dans `config` database
- âœ… RÃ©plication entre les config servers
- âœ… ConnectivitÃ© individuelle de chaque serveur

## ğŸ“š Ressources

- [Documentation MongoDB Sharding](https://docs.mongodb.com/manual/sharding/)
- [MongoDB Replica Sets](https://docs.mongodb.com/manual/replication/)
- [mongos Router](https://docs.mongodb.com/manual/core/sharded-cluster-query-router/)

---

## âš ï¸ Notes importantes

1. **Config Server** : Ne jamais supprimer le config server, il contient toutes les mÃ©tadonnÃ©es du cluster
2. **Routeurs** : Toujours passer par les routeurs (mongos) pour les opÃ©rations sur le cluster shardÃ©
3. **Backup** : Sauvegarder rÃ©guliÃ¨rement les donnÃ©es et le config server
4. **Production** : En production, activer l'authentification et TLS/SSL

---

## ğŸ› DÃ©pannage

### Les containers s'arrÃªtent immÃ©diatement
```powershell
# Voir les logs d'erreur
docker-compose logs <nom_service>

# VÃ©rifier que les ports ne sont pas dÃ©jÃ  utilisÃ©s
netstat -ano | findstr "27017"
```

### Le sharding ne fonctionne pas
```powershell
# VÃ©rifier que les shards sont ajoutÃ©s
docker exec -it routeur_1 mongosh --eval "sh.status()"

# Relancer l'initialisation du sharding
docker-compose restart setup-sharding
docker-compose logs setup-sharding
```

### RÃ©initialiser complÃ¨tement le cluster
```powershell
docker-compose down -v
Remove-Item -Recurse -Force .\*\data\*
docker-compose up -d
```
